{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get top Video Games through Web Scraping, a raw file and an API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABSTRACT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working on a video game dataset and will be extracting video game data from 3 different data sources and then will be munging them together to form a consistent dataset. We will perform several operations over the dataset extracted to make the data clean and error free and consistent. After that we will be developing a database from using the extracted source data and display it in the form of an Entity-Relationship Diagram.\n",
    "The dataset used is the vgchartz.com. It contains details about Video Game ratings, genres, publisher, year of release, description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA                                                                                                    \n",
    "Data gathered is from all the three sources are as follows. Data consists of                                                \n",
    "Id     \n",
    "\n",
    "Name     \n",
    "\n",
    "Description   \n",
    "\n",
    "Genre\n",
    "\n",
    "Publisher\n",
    "\n",
    "Year\n",
    "\n",
    "Platform\n",
    "\n",
    "RAWG Score\n",
    "\n",
    "Critic_Score                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import rawgpy\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SOURCE 1: Using Web Scraping using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Beautiful Soup?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The site we are going to use is http://www.vgchartz.com/gamedb/games.php?name=&keyword=&console=&region=All&developer=&publisher=&goty_year=&genre=&boxart=Both&banner=Both&ownership=Both&showmultiplat=Yes&results=200&order=Sales&showtotalsales=0&showpublisher=0&showpublisher=1&showvgchartzscore=0&shownasales=0&showdeveloper=0&showcriticscore=0&showpalsales=0&showreleasedate=0&showuserscore=0&showjapansales=0&showlastupdate=0&showothersales=0&showshipped=0. Please visit the link to get information on what is being scrapped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the tags from the website\n",
    "url = 'http://www.vgchartz.com/gamedb/games.php?name=&keyword=&console=&region=All&developer=&publisher=&goty_year=&genre=&boxart=Both&banner=Both&ownership=Both&showmultiplat=Yes&results=200&order=Sales&showtotalsales=0&showpublisher=0&showpublisher=1&showvgchartzscore=0&shownasales=0&showdeveloper=0&showcriticscore=0&showpalsales=0&showreleasedate=0&showuserscore=0&showjapansales=0&showlastupdate=0&showothersales=0&showshipped=0'\n",
    "html = requests.get(url, headers = {'User-Agent':'Mozilla/5.0'})\n",
    "print(html.status_code) \n",
    "# Printing the status code, 200 means the request has succeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the above format to more readable format using html parser\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be extracting Rank, Name, Platform and Publisher from vgchartz.com \n",
    "k=[] \n",
    "rank=[]\n",
    "gname=[]\n",
    "publisher=[]\n",
    "platform=[]\n",
    "release_date=[]\n",
    "# Retrieving values using a for loop\n",
    "for tag in soup.find_all('a'):\n",
    "    if tag['href'].startswith('http://www.vgchartz.com/game/'):\n",
    "        k.append(tag.get_text().strip())\n",
    "        data=tag.parent.parent.find_all('td')\n",
    "        if data!=[]:\n",
    "            rank.append(np.int32(data[0].string))\n",
    "            platform.append(data[3].find('img').attrs['alt'].strip(' '))\n",
    "            publisher.append(data[4].string.strip(' '))\n",
    "gname = k[10:] # Our data starts from index position 10 onwards\n",
    "# Creating a dictionary to store the column names for the dataframe\n",
    "columns = {\n",
    "    'Id': rank,\n",
    "    'Name':gname,\n",
    "    'Platform':platform,\n",
    "    'Publisher':publisher\n",
    "}\n",
    "df = pd.DataFrame(columns) # Creating a dataframe with column names Rank, Name, Platform and Publisher.\n",
    "df = df[[\n",
    "    'Id', 'Name', 'Platform',\n",
    "    'Publisher']]\n",
    "# Saving the obtained dataframe on a file named vgsales.csv\n",
    "df.to_csv(\"vgsales.csv\", sep=\",\", encoding='utf-8', index=False) # Saves the data to .csv file\n",
    "#df.drop_duplicates(subset =\"Name\",keep = False, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the information of the top first rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gname[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().any())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASOURCE 2 - Using Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('vgsales2019.csv')\n",
    "#df2.drop(['Rank','ESRB_Rating','Platform','Publisher','Developer','User_Score', 'Total_Shipped', 'Global_Sales', 'NA_Sales', 'PAL_Sales', 'JP_Sales', 'Other_Sales'], axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the information of the data - data type and total number of records in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing, NaN data in the dataframe through CSV\n",
    "df2.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the total null values in the column using sum() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the columns present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['Rank'],axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two dataframe df and dframe having unique names\n",
    "gameData = pd.merge(df,df2, \n",
    "                 on = 'Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.to_csv(\"GameData.csv\",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gameData.drop_duplicates(subset =\"Name\",keep = \"first\", inplace = True)\n",
    "gameData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gameData)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASOURCE 3 - Using API\n",
    "\n",
    "#### What is an API:\n",
    "\n",
    "API stands for Application Programming Interface, and it lets developers integrate any two parts of an application or any different applications together. It consists of various elements such as functions, protocols, and tools that allow developers to build applications. A common goal of all types of APIs is to accelerate the development of applications by providing a part of its functionality out-of-the-box, so developers do not have to implement it themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using an API Wrapper to get data from the video game database www.rawg.io. Since it's a public database we won't be requiring an API key for using www.rawg.io's API.\n",
    "We will be importing the API wrapper python class for www.rawg.io, which is rawgpy.\n",
    "https://rawgpy.readthedocs.io/en/latest/ https://www.shanelynn.ie/merge-join-dataframes-python-pandas-index-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries rawgpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rawgpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be using the data obtained from the Web Scraping to search through rawgpy.io to get ratings and description of the game. The ratings will be on a scale of 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the names of the game on gname\n",
    "gname = gameData['Name']\n",
    "# First initiate two empty list for storing the raw data\n",
    "rawg_ratings = []\n",
    "description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawg = rawgpy.RAWG(\"User-Agent, this should identify your app\")\n",
    "for name in gname:\n",
    "    results = rawg.search(name)  # defaults to returning the top 5 results\n",
    "    game = results[0] # selects the first result of the search\n",
    "    game.populate() # get additional information on the game\n",
    "    rawg_ratings.append(game.rating)\n",
    "# Since description has <p> and </p>, we will replace it with \"\"\n",
    "    desc = game.description.replace(\"</p>\",\"\")\n",
    "    description.append(desc.replace(\"<p>\",\"\"))\n",
    "print (len(rawg_ratings))\n",
    "print(len(description))\n",
    "# It will take 2-3 minutes to complete the iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData[\"RAWG Score\"] = rawg_ratings\n",
    "gameData[\"Description\"] = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and auditing the data in gameData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing, NaN data in the dataframe \n",
    "gameData.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the total null values in the column using sum() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping all columns which have large numbers of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.drop(['ESRB_Rating','Platform_x', 'Publisher_x','Developer',\n",
    "          'User_Score', 'Total_Shipped', 'Global_Sales', 'NA_Sales', \n",
    "          'PAL_Sales', 'JP_Sales', 'Other_Sales'], axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataframe to a temporary file called temp.csv\n",
    "The program isn't able to detect any null values in description attribute. So, we well save the dataset to a temp.csv file and then we will importing it and will be saving it on gameData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.to_csv('temp.csv', encoding = 'utf-8', index = False)\n",
    "# Storing the file in gameData\n",
    "gameData = pd.read_csv('temp.csv') \n",
    "# Checking the total number of null values\n",
    "gameData.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all records having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "# Checking the null values\n",
    "gameData.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the entities present in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that attribute Rank is unique or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData['Id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the information of the data - data type and total number of records in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique to use visualization to understand the data better\n",
    "\n",
    "Here we are using seaborn, matplotlib and scipy to analyse our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing liabries used for visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the Critic_Score and RAWG Score from our dataframe to illustrate how the vote count is distributed in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = gameData['Critic_Score']\n",
    "\n",
    "sns.distplot(critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the result we can see that the Critic Score is not normally distributed. Most of the scores lie between 8 and 9.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawg_score = gameData['RAWG Score']\n",
    "sns.distplot(rawg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here also, the we can see that RAWG Score is not normally distributed. Most of the games in the dataframe have a score of 4.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(np.array(gameData['Critic_Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(np.array(gameData['RAWG Score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCEPTUAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Tables by reformatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a Game_Ratings table using the below columns\n",
    "Game_Ratings = gameData.loc[:,['Id', 'RAWG Score', 'Critic_Score']]\n",
    "Game_Ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a table platform details \n",
    "columns = {'Id': gameData['Id'],\n",
    "          'Platform': gameData['Platform_y']}\n",
    "Platform = pd.DataFrame(columns)\n",
    "Platform.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a table Game Details\n",
    "columns = {'Id': gameData['Id'],'Name': gameData['Name'],'Description': \n",
    "               gameData[\"Description\"],'Genre': \n",
    "               gameData[\"Genre\"], 'Publisher': gameData['Publisher_y'],\n",
    "           'Year':gameData['Year']}\n",
    "Game_Details = pd.DataFrame(columns)\n",
    "Game_Details.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating CSV files (Exporting the files to the PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game_Ratings.to_csv('Game_Ratings.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ratings = pd.read_csv('Game_Ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game_Details.to_csv(\"Game_Details.csv\", encoding = 'utf-8', index = False)\n",
    "game_details_ = pd.read_csv('Game_Details.csv')\n",
    "game_details_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Platform.to_csv(\"Platform.csv\", encoding = 'utf-8', index = False)\n",
    "platform = pd.read_csv('Platform.csv')\n",
    "platform.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_details = pd.merge(game_details_, \n",
    "                     platform,on = 'Id')\n",
    "game_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the two datasets over a common key(id) and the data remains consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_details_combined = pd.merge(game_details_, \n",
    "                                 game_ratings,on='Id')\n",
    "game_details_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('VideoGameERD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows us an entity-relationship model of three different tables from the following: data from game title table,data from ratings table, data from platform table and data from other details. We can infer that id acts as a primary key, meaning that all the tables are linked with game_title. The fields title,overview,release_date,original_title\n",
    "from movie_title. id holds the common data in movietitle and language. Hence, we can conclude that on merging the three tables we obtain one table which defines a combined data set of values from different places yet relatable, which is known as a conceptual database schema.\n",
    "\n",
    "### AUDIT VALIDITY/ACCURACY\n",
    "\n",
    "We say data is accurate only when it is neat and with no null or junk values. By using drop function, all the unwanted null values were deleted from the above rows and columns which gives a report on valid and accurate data. We have also dropped some of the columns which had large number of null values.\n",
    "\n",
    "### AUDIT COMPLETNESS\n",
    "\n",
    "In real world, when a list of video games from a particular customer is requested, a list of it will be displayed or presented, similarly when we compare it with above data too, we get proper real time data showing correct information for all the video games.  \n",
    "\n",
    "\n",
    "### AUDIT CONSISTENCY/UNIFORMITY\n",
    "\n",
    "The datasets which have been used in this assignment show a uniform relationship between each of the dataset since they are linked to each other by a common attribute. \n",
    "\n",
    "### REPORT\n",
    "\n",
    "fies used : vgsales2019.csv                                                                         \n",
    "files genearted:  Platform.csv, Game_Ratings.csv,Game_Details.csv                                      \n",
    "Data is reformatted to fit into a conceptual model. Data gathered from different sources Web API, Web scraping, Raw file and are mergerd together to fit into a conceptual model.                                              \n",
    "\n",
    "Code used:                                                                                                 \n",
    "Step 1. Extraction of Data                                                                                    \n",
    "3 main methods were used for the extraction of data:                                                        \n",
    "1. Using the API Wrapper:                                                                                             \n",
    "Here since the rawg.io is a public video game database, we won't be neeeding an API key. We will be using an API Wrapper to get the RAWG Score of each game we scraped from the website vgsales.com. We will be importing the Python API wrapper class rawgpy so as to do the same process as an API.                                                                    \n",
    "pandas to create data frames from the raw data                                                                 \n",
    "2. Using the website to scrap the data                                                                       \n",
    "Here the data was extracted using the sites data directly using the libraries like:                               \n",
    "request to access the website using the URL (http://www.vgchartz.com/gamedb/games.php?name=&keyword=&console=&region=All&developer=&publisher=&goty_year=&genre=&boxart=Both&banner=Both&ownership=Both&showmultiplat=Yes&results=200&order=Sales&showtotalsales=0&showpublisher=0&showpublisher=1&showvgchartzscore=0&shownasales=0&showdeveloper=0&showcriticscore=0&showpalsales=0&showreleasedate=0&showuserscore=0&showjapansales=0&showlastupdate=0&showothersales=0&showshipped=0)                       \n",
    "BeautifulSoup to scrape the contents of the website                                                          \n",
    " find_all() and parent methods were used to find the desired content in the system                              \n",
    "3. By loading the csv file:                                                                                  \n",
    "Here the data was extracted using a csv file on the system using the libraries like:                            \n",
    "Pandas to a read the csv file and load it into data frames                                                    \n",
    "read_csv method is used to read.csv file                                                                   \n",
    "Step 2. Cleaning and Auditing Data                                                                             \n",
    "To gain knowledge about the dataset we used various methods like                                                  \n",
    "describe, isnull, any, shape, columns, is_unique, info, iloc, loc, os\n",
    "\n",
    "\n",
    "Code used for Merge  1                                                                                         \n",
    "game_details = pd.merge(game_details, \n",
    "                     platform,on = 'Id')                                             \n",
    "                                                                                                 \n",
    "Code used for final merge                                                                                    \n",
    "game_details_combined = pd.merge(game_details_, \n",
    "                                 game_ratings,on='Id')\n",
    "\n",
    "\n",
    "### CONCLUSION                                                                                                                                                                   \n",
    "Primary focus of this assignment is to learn how to get the data from different sources, cleaning of data, checking null values present in the data, data munging and to reformat the data to fit a conceptual database model.\n",
    "\n",
    "### CONTRIBUTION\n",
    "###### Your contribution towards project. How much code did you write and how much you took from other site or some other source.                                                                            \n",
    "Ashwin John Chempolil: 15% \n",
    "\n",
    "Crispin Sujith Cletus: 10%\n",
    "\n",
    "By External source: 50%                                                                                        \n",
    "\n",
    "Provided by the professor : 25% \n",
    "\n",
    "\n",
    "### CITATIONS\n",
    "##### Sources from where you have gained knowledge or used codes, data. It may include Web links, github links, code taken from somewhere etc.\n",
    "http://www.vgchartz.com/gamedb/games.php?name=&keyword=&console=&region=All&developer=&publisher=&goty_year=&genre=&boxart=Both&banner=Both&ownership=Both&showmultiplat=Yes&results=200&order=Sales&showtotalsales=0&showpublisher=0&showpublisher=1&showvgchartzscore=0&shownasales=0&showdeveloper=0&showcriticscore=0&showpalsales=0&showreleasedate=0&showuserscore=0&showjapansales=0&showlastupdate=0&showothersales=0&showshipped=0                                                              \n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/           https://github.com/GregorUT/vgchartzScrape/blob/master/vgchartzfull.py                        \n",
    "https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html\n",
    "https://thispointer.com/python-pandas-how-to-drop-rows-in-dataframe-by-index-labels/\n",
    "https://rawgpy.readthedocs.io/en/latest/\n",
    "https://www.shanelynn.ie/merge-join-dataframes-python-pandas-index-1/\n",
    "\n",
    "### LICENSE\n",
    "Copyright 2019 Ashwin John Chempolil, Crispin Sujith Cletus\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated \n",
    "documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the \n",
    "rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n",
    "persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the \n",
    "Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE \n",
    "WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR \n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR \n",
    "OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
